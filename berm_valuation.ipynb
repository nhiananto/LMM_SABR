{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bermudan Swaptions Valuation\n",
    "Valuation of semi-annual fixed & floating leg bermudan swaptions under the LSM framework using Neural Net and XGBoost instead of least-squares to compute the conditional expected continuation value at each time step. Simulated rates used for valuing the bermudan swaptions are obtained from \"main_LMM_SABR\" and the results from that is stored under the \"simulation_results\".\n",
    "\n",
    "The example in the main_LMM_SABR notebook only generates 1k simulated paths. \\\n",
    "10k simulated paths are given in the folder as an example by running the main_LMM_SABR and changing the number of simulations parameter.\n",
    "\n",
    "## Need:\n",
    "- simulated zeros paths (from main_LMM_SABR) \\\n",
    "generate the amount of paths you want and load the paths here\n",
    "- berm_swaptions class (from berm_swaptions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import scipy\n",
    "from sklearn import model_selection, metrics\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import date\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from berm_swaption_class import * #import berm_swaptions & berm_swaptions_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load rates sample dataset\n",
    "'''\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "path_rates = os.path.join(os.getcwd(), \"simulation_results\")\n",
    "path_valuation_results = os.path.join(os.getcwd(), \"valuation_results\")\n",
    "path_tune_chk = os.path.join(os.getcwd(), \"tune_checkpoint\") #main path for tune checkpoint and trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rates_1k = load_object(path_rates+\"\\\\simulated_zeros_1k.pkl\")\n",
    "sim_rates_10k = load_object(path_rates+\"\\\\simulated_zeros_10k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 1000)\n",
      "[[0.99884244 0.         0.         0.         0.        ]\n",
      " [0.9976228  0.99854474 0.         0.         0.        ]\n",
      " [0.99739711 0.99706474 0.99810346 0.         0.        ]\n",
      " [0.99730355 0.99641432 0.99606452 0.99800189 0.        ]\n",
      " [0.99679304 0.99587095 0.99580402 0.99592028 0.99851665]]\n",
      "(20, 20, 10000)\n",
      "[[0.99884244 0.         0.         0.         0.        ]\n",
      " [0.9976228  0.99877373 0.         0.         0.        ]\n",
      " [0.99739711 0.99751846 0.99862225 0.         0.        ]\n",
      " [0.99730355 0.99725279 0.99712725 0.99849212 0.        ]\n",
      " [0.99679304 0.99639462 0.99583648 0.99571872 0.99731072]]\n"
     ]
    }
   ],
   "source": [
    "#check sample rate paths\n",
    "print(sim_rates_1k.shape)\n",
    "print(sim_rates_1k[:5,:5,0])\n",
    "\n",
    "print(sim_rates_10k.shape)\n",
    "print(sim_rates_10k[:5,:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list containing all the rates\n",
    "rate_list = [sim_rates_1k, sim_rates_10k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN Model\n",
    "Changeable number of layers/neurons for Bayesian Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# define custom neural net class for bayes opt tuning\n",
    "# =============================================================================\n",
    "class net(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, hidden_layers, activation_fn):\n",
    "        super(net, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.layers= nn.ModuleList()\n",
    "        #add first layer\n",
    "        self.layers.append(nn.Linear(input_dim, neurons))\n",
    "        \n",
    "        for k in range(1, hidden_layers+1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            \n",
    "        #output layer\n",
    "        self.layers.append(nn.Linear(neurons, 1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1): #except last layer\n",
    "            x = self.layers[i](x)\n",
    "            x = self.activation_fn(x)\n",
    "\n",
    "        #output layer (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laguerre polynomials for the basis functions at each step\n",
    "\n",
    "def laguerre_scipy(x, k):\n",
    "    #x: paths x nvars\n",
    "    #return paths x (nvars * polynomials)\n",
    "    \n",
    "    dim = x.shape\n",
    "    L = np.zeros((dim[0], 1)) #paths x 1 #placeholder\n",
    "    \n",
    "    for i in range(dim[1]):\n",
    "        weight = np.exp(-x[:, i]/2)\n",
    "        for j in range(k):\n",
    "            L = np.concatenate([L,\n",
    "                            (weight.reshape(-1,1) * \n",
    "                            scipy.special.eval_genlaguerre(j, 0, x[:,i]).reshape(-1,1))], axis = 1)\n",
    "    \n",
    "    L = L[:,1:]\n",
    "    return(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training NN function using tune to report validation losses for early stopping & bayes-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(config, x, y, input_size, max_num_epochs, use_tune = True,\n",
    "            checkpoint_dir = None):\n",
    "    '''\n",
    "    config = dict of number of neurons, num hidden, activation function used, learning rate, batch size\n",
    "    '''\n",
    "    \n",
    "    model = net(input_size, config[\"num_neurons\"], config[\"num_hidden\"], config[\"activation_fn\"])\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.functional.mse_loss\n",
    "    optimizer=optim.Adam(model.parameters(),lr= config[\"lr\"])\n",
    "    \n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min',\n",
    "                                     patience = 10, threshold_mode = 'rel', threshold = 1e-4)\n",
    "    \n",
    "    \n",
    "    x = x.reshape(-1, input_size)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    #split\n",
    "    x_train,x_test,y_train,y_test = model_selection.train_test_split(x,\n",
    "                                                                    y, test_size=0.1) \n",
    "    \n",
    "    x_train = torch.tensor(x_train,dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "    x_test = torch.tensor(x_test,dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train.view(-1, input_size), y_train.view(-1,1))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test.view(-1, input_size), y_test.view(-1,1))\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "\n",
    "    #track losses\n",
    "    train_losses, test_losses = [], []\n",
    "    test_r2 = []\n",
    "    avg_train_losses, avg_test_losses  = [], []\n",
    "    avg_test_r2  = []\n",
    "    \n",
    "    #set initial losses as inf\n",
    "    avg_train_losses.append(float('inf'))\n",
    "    avg_test_losses.append(float('inf'))\n",
    "    \n",
    "    def accuracy_r2(y_true, y_pred):\n",
    "        #for tensors\n",
    "        y_true = y_true.detach().numpy().reshape(-1,1)\n",
    "        y_pred = y_pred.detach().numpy().reshape(-1,1)\n",
    "        return metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    for epoch in range(max_num_epochs):\n",
    "        # =======================\n",
    "        # train        \n",
    "        # =======================\n",
    "        model.train() #training mode    \n",
    "        for i, (features, price) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, price= features.to(device), price.to(device)\n",
    "            \n",
    "            #forward\n",
    "            model_output = model(features)\n",
    "            \n",
    "            #backprop\n",
    "            loss = criterion(model_output,price)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            \n",
    "        # ======================\n",
    "        # eval\n",
    "        # ======================\n",
    "        model.eval() # prep model for evaluation\n",
    "        for i, (features, price) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "                features = features.to(device)\n",
    "                price = price.to(device)\n",
    "                # forward pass\n",
    "                model_output = model(features)\n",
    "                \n",
    "                # calculate the loss\n",
    "                loss = criterion(model_output, price)\n",
    "                # record validation loss\n",
    "                test_losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "        train_loss, test_loss = np.mean(train_losses), np.mean(test_losses)\n",
    "        \n",
    "\n",
    "        #whole validation set\n",
    "        test_r2 = accuracy_r2(test_dataset.tensors[1], model(test_dataset.tensors[0]))\n",
    "    \n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        avg_test_r2.append(test_r2)\n",
    "        \n",
    "        #lr scheduler min mse\n",
    "        lr_scheduler.step(test_loss)\n",
    "        \n",
    "        \n",
    "        if use_tune:\n",
    "        #    print(\"Saving checkpoint\")\n",
    "        #    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "        #          path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        #          torch.save(model.state_dict(), path) # optimizer.state_dict()\n",
    "                  \n",
    "            tune.report(loss= test_loss, accuracy= test_r2) #report back to tune\n",
    "            \n",
    "            \n",
    "            \n",
    "        #clear list\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "\n",
    "    print(\"Finished Training\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define main NN function that uses tune with ASHAscheduler and BayesianOpt from skopt to try different nn configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# setup environament for ray tune\n",
    "# =============================================================================\n",
    "'''\n",
    "config: dictionary of search space\n",
    "num_neurons = #of neurons for each hidden layer\n",
    "num_hidden = number of hidden layers (not incl. output layer)\n",
    "activation_fn = activation function for each hidden layer\n",
    "batch size = batch size for mini batch GD\n",
    "lr = learning rate for adam\n",
    "'''\n",
    "def main_NN(data, num_samples=15, max_num_epochs=30,\n",
    "            metric = 'loss', mode = 'min',\n",
    "            checkpoint_dir = None,\n",
    "            trial_path = None,\n",
    "            experiment_name = \"experiment\"+str(date.today()),\n",
    "            trial_name = None):\n",
    "    '''\n",
    "    data = tuple of (X, y) dataset used for training and validation\n",
    "    num_samples = samples to search from search space\n",
    "    max_num_epochs = max number of epochs to train the NN\n",
    "    \n",
    "    search over NN hyperspace defined by config\n",
    "    max_num_epochs = max epochs for ASHAScheduler to terminate training\n",
    "    num_samples = num trials\n",
    "    \n",
    "    trial_name= current trial name\n",
    "    trial_dir = same as trial name\n",
    "    \n",
    "    '''\n",
    "    #if not using bayes-opt\n",
    "    \n",
    "    # config = {\n",
    "    #     \"num_neurons\": tune.choice([16, 32, 64, 128]),\n",
    "    #     \"num_hidden\": tune.choice([2,3,4]),\n",
    "    #     \"activation_fn\" : tune.choice([F.relu, F.leaky_relu]),\n",
    "    #     \"batch_size\": tune.choice([32]),\n",
    "    #     \"lr\": tune.loguniform(1e-4, 1e-1)\n",
    "    # }\n",
    "    \n",
    "    bayes_searchspace = {\n",
    "        \"num_neurons\": Categorical([int(x) for x in  2**np.arange(4,8)]),\n",
    "        \"num_hidden\": Integer(2, 4, 'uniform'),\n",
    "        \"activation_fn\" : Categorical([F.relu]),\n",
    "        # \"batch_size\": Integer(16, 32),#Categorical([int(16), int(32)]),\n",
    "        \"lr\": Real(1e-4, 1e-2, 'log-uniform')\n",
    "        }\n",
    "    \n",
    "    \n",
    "    skopt_search = SkOptSearch(space = bayes_searchspace, metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    can set metric/mode in scheduler or tune.run\n",
    "    ASHA scheduler can set max_num_epochs\n",
    "    grace_period = min number of iterations before stopping\n",
    "    '''\n",
    "    scheduler = ASHAScheduler(\n",
    "                    metric= metric, #loss\n",
    "                    mode= mode, #min\n",
    "                    time_attr='training_iteration',\n",
    "                    max_t=max_num_epochs,\n",
    "                    grace_period=10, #set grace period before stopping here\n",
    "                    reduction_factor=2)\n",
    "    \n",
    "    '''\n",
    "    CLIReporter for python console\n",
    "    what to print to console\n",
    "    '''\n",
    "    reporter = JupyterNotebookReporter(overwrite = True, \n",
    "        parameter_columns=[\"lr\", \"num_neurons\", \"num_hidden\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #get dataset\n",
    "    x, y = data\n",
    "    input_size = x.shape[1]\n",
    "    \n",
    "    \n",
    "    def trial_name_string(trial):\n",
    "        return trial_name+str(trial.trial_id)\n",
    "    \n",
    "    result = tune.run(\n",
    "            partial(trainNN, x = x, y = y, input_size = input_size, max_num_epochs = max_num_epochs,\n",
    "                    checkpoint_dir = checkpoint_dir),\n",
    "            resources_per_trial={\"cpu\": 3, \"gpu\": 0},\n",
    "            # config=config,\n",
    "            search_alg = skopt_search,\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors = True,\n",
    "            progress_reporter=reporter,\n",
    "            name= experiment_name,\n",
    "            local_dir = trial_path,\n",
    "            trial_name_creator = trial_name_string,\n",
    "            trial_dirname_creator = trial_name_string)\n",
    "    \n",
    "    #get best trial\n",
    "    best_trial = result.get_best_trial(metric, mode, \"last\") #\"accuracy\" , max\n",
    "    \n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = net(input_size, best_trial.config[\"num_neurons\"],\n",
    "                             best_trial.config[\"num_hidden\"], best_trial.config[\"activation_fn\"])\n",
    "    \n",
    "    \n",
    "    #load best model state dict and optimizer from best checkpoint path\n",
    "    #best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    \n",
    "    #model_state  = torch.load(os.path.join( #optimizer_state\n",
    "    #     checkpoint_dir, \"checkpoint\"))\n",
    "    \n",
    "    \n",
    "    #best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        # if gpus_per_trial > 1:\n",
    "        #     best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    \n",
    "    return best_trained_model, result, best_trial.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrainNN(x, y, config, max_num_epochs):\n",
    "    '''\n",
    "    config = dict of number of neurons, num hidden, activation function used, learning rate, batch size\n",
    "    '''\n",
    "    \n",
    "    input_size = x.shape[1]\n",
    "    \n",
    "    model = net(input_size, config[\"num_neurons\"], config[\"num_hidden\"], config[\"activation_fn\"])\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = torch.nn.functional.mse_loss\n",
    "    optimizer=optim.Adam(model.parameters(),lr= config[\"lr\"])\n",
    "    \n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min',\n",
    "                                     patience = 10, threshold_mode = 'rel', threshold = 1e-4)\n",
    "    \n",
    "    x = x.reshape(-1, input_size)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    #split\n",
    "    x_train,x_test,y_train,y_test = model_selection.train_test_split(x,\n",
    "                                                                    y, test_size=0.1) \n",
    "    \n",
    "    x_train = torch.tensor(x_train,dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "    x_test = torch.tensor(x_test,dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train.view(-1, input_size), y_train.view(-1,1))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test.view(-1, input_size), y_test.view(-1,1))\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "\n",
    "    #track losses\n",
    "    train_losses, test_losses = [], []\n",
    "    test_r2 = []\n",
    "    avg_train_losses, avg_test_losses  = [], []\n",
    "    avg_test_r2  = []\n",
    "    \n",
    "    #set initial losses as inf\n",
    "    avg_train_losses.append(float('inf'))\n",
    "    avg_test_losses.append(float('inf'))\n",
    "    \n",
    "    def accuracy_r2(y_true, y_pred):\n",
    "        #for tensors\n",
    "        y_true = y_true.detach().numpy().reshape(-1,1)\n",
    "        y_pred = y_pred.detach().numpy().reshape(-1,1)\n",
    "        return metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    for epoch in range(max_num_epochs):\n",
    "        print(epoch)\n",
    "        # =======================\n",
    "        # train        \n",
    "        # =======================\n",
    "        model.train() #training mode    \n",
    "        for i, (features, price) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, price= features.to(device), price.to(device)\n",
    "            \n",
    "            #forward\n",
    "            model_output = model(features)\n",
    "            \n",
    "            #backprop\n",
    "            loss = criterion(model_output,price)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            \n",
    "        # ======================\n",
    "        # eval\n",
    "        # ======================\n",
    "        model.eval() # prep model for evaluation\n",
    "        for i, (features, price) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "                features = features.to(device)\n",
    "                price = price.to(device)\n",
    "                # forward pass\n",
    "                model_output = model(features)\n",
    "                \n",
    "                # calculate the loss\n",
    "                loss = criterion(model_output, price)\n",
    "                # record validation loss\n",
    "                test_losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "        train_loss, test_loss = np.mean(train_losses), np.mean(test_losses)\n",
    "\n",
    "        \n",
    "        #whole validation set\n",
    "        test_r2 = accuracy_r2(test_dataset.tensors[1], model(test_dataset.tensors[0]))\n",
    "    \n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        avg_test_r2.append(test_r2)\n",
    "        \n",
    "        #lr scheduler min mse\n",
    "        lr_scheduler.step(test_loss)\n",
    "        \n",
    "    return model, model.state_dict(), optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with Neural Network\n",
    "Set up environment and train hyperparameters for nn using config\n",
    "Use simulated interest rate model\n",
    "loop through exercise steps to calculate continuation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wrapper function for neural network framework to calculate out-of-sample results\n",
    "'''\n",
    "def oos_nn(berm_swaptions, model, step, laguerre = False):\n",
    "    '''\n",
    "    berm_swaptions = OOS bermudan swaptions class\n",
    "    model = neural net model\n",
    "    '''\n",
    "    itm = np.where(berm_swaptions.intrinsic[:, step] > 0)[0]\n",
    "    \n",
    "    X = berm_swaptions.X(step)[0]\n",
    "    initial_dim = X.shape\n",
    "    \n",
    "    X = X[itm, :]\n",
    "    \n",
    "    if laguerre:\n",
    "        X = laguerre_scipy(X, 3)\n",
    "    \n",
    "    y = berm_swaptions.value[:, step+1]\n",
    "    y = berm_swaptions.params[\"rate_matrix\"][step, step, itm] * y[itm] #discount to now\n",
    "    \n",
    "    pred = model(torch.tensor(X, dtype = torch.float)).detach().numpy().reshape(-1,1)\n",
    "    \n",
    "    pred_all = np.zeros((initial_dim[0], 1))\n",
    "    pred_all[itm, :] = pred\n",
    "    \n",
    "    #update\n",
    "    berm_swaptions.update(step, pred_all.ravel())\n",
    "    \n",
    "    #oos r2\n",
    "    r2_score = metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1))\n",
    "    return(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "main valuation function for neural network\n",
    "'''\n",
    "def berm_nn(berm_swaptions, sim_rates, strike, lockout, tenor, opt_type = \"rec\", max_num_epochs = 30,\n",
    "                num_samples = 10, metric = 'loss', mode = 'min',\n",
    "                test_size = 0.1, seed = 123, trial_path = None, checkpoint_dir = None):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    max_num_epochs = maximum number of epochs before stopping\n",
    "    num_samples = number of samples taken for bayesopt\n",
    "    opt_type = \"rec\"/\"pay\" for receiver or payer\n",
    "    sim_rates : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "      \n",
    "    tune_result_list = []\n",
    "    model_list = []\n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    sampling_increase = 0#np.cumsum(steps%3)\n",
    "    total_time = 0\n",
    "    ct = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # NN model\n",
    "    \n",
    "   \n",
    "        _, tune_result, config = main_NN((X, y),\n",
    "                                    num_samples = int(num_samples + 0),\n",
    "                                    max_num_epochs = max_num_epochs,\n",
    "                           trial_path = trial_path+\"\\\\trials\",\n",
    "                           checkpoint_dir = checkpoint_dir,\n",
    "                           experiment_name = \"nn_step_\"+str(i), trial_name = \"nn\",\n",
    "                           metric = metric, mode = mode)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #retrain model with best config\n",
    "        model, _, _ = retrainNN(X, y, config, max_num_epochs)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        \n",
    "        #valuation\n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += (t1 - t0)\n",
    "        \n",
    "        pred = model(torch.tensor(X, dtype =  torch.float)).detach().numpy().reshape(-1,1)\n",
    "        pred_all = np.zeros((training_paths , 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "\n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        print(\"Step: \" + str(i))\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        \n",
    "        \n",
    "        oos_r2[\"Accuracy\"].append(oos_nn(berm_swaptions = sim_oos,\n",
    "                                           model = model,\n",
    "                                           step = i, laguerre = True)) #append r2 and update\n",
    "        ct += 1\n",
    "        \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": steps,\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, steps], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, steps], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    print(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    print(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    print(price_oos)\n",
    "    \n",
    "    return (price, price_oos, results_table, total_time, model_list, tune_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.7 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 20.000: None | Iter 10.000: None<br>Resources requested: 3/12 CPUs, 0/1 GPUs, 0.0/4.44 GiB heap, 0.0/1.51 GiB objects<br>Result logdir: C:\\Users\\nhian\\Dropbox\\My PC (DESKTOP-L6D69LH)\\Documents\\GitHub\\LMM_SABR\\tune_checkpoint\\trials\\nn_step_11<br>Number of trials: 1/5 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name  </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_neurons</th><th style=\"text-align: right;\">  num_hidden</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>nn81f3b676  </td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.000721287</td><td style=\"text-align: right;\">           64</td><td style=\"text-align: right;\">           3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "value bermudan swaptions using NN at each step from the rate_list\n",
    "'''\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "nn_results_1_10 = []\n",
    "for i in rate_list:\n",
    "    nn_results_1_10.append(berm_nn(berm_swaptions, i, strike = 0.0015, lockout = 1, #1 into 10, 0.0015 as strike\n",
    "                               tenor = 10, opt_type = \"rec\",\n",
    "                               num_samples = 5, max_num_epochs = 25, test_size = 0.1, seed = 123, trial_path = path_tune_chk,\n",
    "                               checkpoint_dir = path_tune_chk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with Linear Regression\n",
    "### For Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper function for calculation OOS values for XGB and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oos_pred(berm_swaptions, model, step, laguerre = False):\n",
    "    '''\n",
    "    berm_swaptions = OOS bermudan swaptions class\n",
    "    model = out model with predict method (X,y)\n",
    "    '''\n",
    "    itm = np.where(berm_swaptions.intrinsic[:, step] > 0)[0]\n",
    "    \n",
    "    X = berm_swaptions.X(step)[0]\n",
    "    initial_dim = X.shape\n",
    "    \n",
    "    X = X[itm, :]\n",
    "    \n",
    "    if laguerre:\n",
    "        X = laguerre_scipy(X, 3)\n",
    "    \n",
    "    y = berm_swaptions.value[:, step+1]\n",
    "    y = berm_swaptions.params[\"rate_matrix\"][step, step, itm] * y[itm] #discount to now\n",
    "    \n",
    "    pred = model.predict(X).reshape(-1,1)\n",
    "    \n",
    "    pred_all = np.zeros((initial_dim[0], 1))\n",
    "    pred_all[itm, :] = pred\n",
    "    \n",
    "    #update\n",
    "    berm_swaptions.update(step, pred_all.ravel())\n",
    "    \n",
    "    #oos r2\n",
    "    r2_score = metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1))\n",
    "    return(r2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berm_lr(berm_swaptions, sim_rates, strike, lockout, tenor, opt_type = \"rec\",\n",
    "                test_size = 0.1, seed = 123, verbose = False):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    sim_zeros : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "      \n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    total_time = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # model\n",
    "        \n",
    "        out = LinearRegression().fit(X, y.reshape(-1,1))\n",
    "        pred= out.predict(X).reshape(-1,1)\n",
    "        \n",
    "        pred_all = np.zeros((training_paths , 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += (t1 - t0)\n",
    "        \n",
    "        \n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(out.predict(X).reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        verboseprint(\"Step: \" + str(i))\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        oos_r2[\"Accuracy\"].append(oos_pred(berm_swaptions = sim_oos,\n",
    "                                           model = out,\n",
    "                                           step = i, laguerre = True)) #append r2 and update\n",
    "        \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": steps,\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, steps], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, steps], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    verboseprint(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    verboseprint(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    verboseprint(price_oos)\n",
    "    \n",
    "    return (price, price_oos, results_table, total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LR simulation for all rate paths contained in rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results_1_10 = []\n",
    "\n",
    "for i in rate_list:\n",
    "    lr_results_1_10.append(berm_lr(berm_swaptions = berm_swaptions,\n",
    "                              sim_rates = i, strike = 0.0015, lockout = 1, tenor = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with XGBoost\n",
    "### Bayesian Opt using skopt\n",
    "Bayesian optimization at each step \\\n",
    "Takes considerable amount of time to run ~1-2 hours depending on number of paths and steps estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berm_xgb(berm_swaptions, sim_rates, strike, lockout, tenor, test_size = 0.1,\n",
    "             search_iter = 15, cv = 3, search_scoring = 'neg_mean_squared_error',\n",
    "             seed = 123, verbose = False):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    sim_zeros : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "     \n",
    "    model_list = []\n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    total_time = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # model\n",
    "        \n",
    "        model = XGBRegressor()\n",
    "        \n",
    "        param_test = {\n",
    "                'learning_rate': Real(0.01, 0.75, 'log-uniform'),\n",
    "                'min_child_weight': Integer(0, 10, 'uniform'),\n",
    "                'max_depth': Integer(3, 35, 'uniform'),\n",
    "                'max_delta_step': Integer(0, 20),\n",
    "                'subsample': Real(0.1, 1.0, 'uniform'),\n",
    "                'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "                'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "                'reg_lambda': Real(1e-9, 10, 'log-uniform'),\n",
    "                'reg_alpha': Real(1e-9, 1e-2, 'log-uniform'),\n",
    "                'gamma': Real(1e-9, 1e-3, 'log-uniform'), # minsplit loss\n",
    "                'n_estimators': Integer(125, 350)\n",
    "                }\n",
    "        \n",
    "        \n",
    "        gsearch = BayesSearchCV(estimator = model, n_iter = search_iter,\n",
    "                              search_spaces= param_test,\n",
    "                              scoring= search_scoring, cv=cv, refit = True, random_state = seed)\n",
    "        \n",
    "        \n",
    "        search_res = gsearch.fit(X, y)\n",
    "        out = gsearch.best_estimator_ #XGBRegressor(**gsearch.best_params_).fit(X,y)\n",
    "        \n",
    "        model_list.append(gsearch.best_params_)\n",
    "        \n",
    "        pred = out.predict(X).reshape(-1,1)\n",
    "        pred_all = np.zeros((training_paths, 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += t1 - t0\n",
    "        \n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(out.predict(X).reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        #update\n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        verboseprint(\"Step: \" + str(i))\n",
    "        verboseprint(best_results_r2[\"Accuracy\"][-1])\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        #update\n",
    "        oos_r2[\"Accuracy\"].append(oos_pred(berm_swaptions = sim_oos,\n",
    "                                           model = out,\n",
    "                                           step = i, laguerre = True)) #append r2 and update, use laguerre expansion\n",
    "    \n",
    "    \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": np.flip(steps),\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, np.flip(steps)], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, np.flip(steps)], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    \n",
    "   \n",
    "    verboseprint(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    verboseprint(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    verboseprint(price_oos)\n",
    "    \n",
    "    \n",
    "    return (price, price_oos, results_table, total_time, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18\n",
      "0.9852836064910683\n",
      "Step: 17\n",
      "0.9949699280563477\n",
      "Step: 16\n",
      "0.995714533890012\n",
      "Step: 15\n",
      "0.9961352470847176\n",
      "Step: 14\n",
      "0.9962072898793132\n",
      "Step: 13\n",
      "0.9960149059095496\n",
      "Step: 12\n",
      "0.9949672179097943\n",
      "Step: 11\n",
      "0.9942850099635865\n",
      "Step: 10\n",
      "0.9959695496760629\n",
      "Step: 9\n",
      "0.7666032549031567\n",
      "Step: 8\n",
      "0.2174570012412309\n",
      "Step: 7\n",
      "-0.028553349402233863\n",
      "Step: 6\n",
      "-0.32595587531823433\n",
      "Step: 5\n",
      "0.6584164867068752\n",
      "Step: 4\n",
      "-0.8496527159560869\n",
      "Step: 3\n",
      "-1.8287200925857743\n",
      "Step: 2\n",
      "-1.4413601802637395\n",
      "    Step  Training R2    OOS R2    Exc Pr  Exc Pr OOS\n",
      "0      2     0.985284  0.962016  0.218889        0.17\n",
      "1      3     0.994970  0.983059  0.122222        0.12\n",
      "2      4     0.995715  0.949316  0.018889        0.02\n",
      "3      5     0.996135  0.895410  0.075556        0.10\n",
      "4      6     0.996207  0.771292  0.076667        0.02\n",
      "5      7     0.996015  0.678754  0.010000        0.01\n",
      "6      8     0.994967  0.416784  0.003333        0.01\n",
      "7      9     0.994285 -0.014584  0.020000        0.00\n",
      "8     10     0.995970  0.222887  0.027778        0.05\n",
      "9     11     0.766603  0.060807  0.008889        0.00\n",
      "10    12     0.217457 -0.465758  0.017778        0.02\n",
      "11    13    -0.028553 -0.618705  0.012222        0.00\n",
      "12    14    -0.325956 -1.652677  0.011111        0.00\n",
      "13    15     0.658416 -0.440217  0.011111        0.01\n",
      "14    16    -0.849653 -1.105003  0.007778        0.02\n",
      "15    17    -1.828720 -1.573564  0.012222        0.00\n",
      "16    18    -1.441360 -2.215862  0.025556        0.02\n",
      "0.005138202794209562\n",
      "0.0028912948803467963\n",
      "Step: 18\n",
      "0.9831848006029723\n",
      "Step: 17\n",
      "0.9971103377963453\n",
      "Step: 16\n",
      "0.9961624670315802\n",
      "Step: 15\n",
      "0.9832018018334138\n",
      "Step: 14\n",
      "0.9658966354040263\n",
      "Step: 13\n",
      "0.9309041063048462\n",
      "Step: 12\n",
      "0.8798764254257994\n",
      "Step: 11\n",
      "0.8253463791823573\n",
      "Step: 10\n",
      "0.660280453694524\n",
      "Step: 9\n",
      "0.5786598900326165\n",
      "Step: 8\n",
      "0.48835484701265175\n",
      "Step: 7\n",
      "0.4228312697193153\n",
      "Step: 6\n",
      "0.25479989355666655\n",
      "Step: 5\n",
      "0.15747617956869053\n",
      "Step: 4\n",
      "-0.04830518988331112\n",
      "Step: 3\n",
      "-0.3668926520299418\n",
      "Step: 2\n",
      "-0.7404306832081762\n",
      "    Step  Training R2    OOS R2    Exc Pr  Exc Pr OOS\n",
      "0      2     0.983185  0.959955  0.268889       0.209\n",
      "1      3     0.997110  0.986734  0.174333       0.175\n",
      "2      4     0.996162  0.984414  0.074000       0.106\n",
      "3      5     0.983202  0.968400  0.015111       0.016\n",
      "4      6     0.965897  0.933858  0.049000       0.050\n",
      "5      7     0.930904  0.899513  0.012556       0.021\n",
      "6      8     0.879876  0.819671  0.020333       0.023\n",
      "7      9     0.825346  0.719726  0.006000       0.005\n",
      "8     10     0.660280  0.605128  0.012333       0.020\n",
      "9     11     0.578660  0.439547  0.005000       0.007\n",
      "10    12     0.488355  0.349486  0.016333       0.011\n",
      "11    13     0.422831  0.241395  0.004111       0.003\n",
      "12    14     0.254800  0.072496  0.014222       0.015\n",
      "13    15     0.157476 -0.193906  0.005556       0.002\n",
      "14    16    -0.048305 -0.222335  0.008556       0.009\n",
      "15    17    -0.366893 -0.417866  0.005222       0.009\n",
      "16    18    -0.740431 -0.760337  0.026667       0.013\n",
      "0.004816259307637045\n",
      "0.0041849371230658974\n"
     ]
    }
   ],
   "source": [
    "xgb_results_1_10 = []\n",
    "\n",
    "for i in rate_list:\n",
    "    xgb_results_1_10.append(berm_xgb(berm_swaptions = berm_swaptions,\n",
    "                              sim_rates = i, strike = 0.0015, lockout = 1, tenor = 10, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save run results\n",
    "save_object(lr_results_1_10, path_valuation_results+\"\\\\lr_results_1_10.pkl\")\n",
    "\n",
    "save_object(nn_results_1_10, path_valuation_results+\"\\\\nn_results_1_10.pkl\")\n",
    "\n",
    "save_object(xgb_results_1_10, path_valuation_results+\"\\\\xgb_results_1_10.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
