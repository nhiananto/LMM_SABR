{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bermudan Swaptions Valuation\n",
    "Valuation of semi-annual fixed & floating leg bermudan swaptions under the LSM framework using Neural Net and XGBoost instead of least-squares to compute the conditional expected continuation value at each time step. Simulated rates used for valuing the bermudan swaptions are obtained from \"main_LMM_SABR\" and the results from that is stored under the \"simulation_results\".\n",
    "\n",
    "The example in the main_LMM_SABR notebook only generates 1k simulated paths. \\\n",
    "10k simulated paths are given in the folder as an example by running the main_LMM_SABR and changing the number of simulations parameter.\n",
    "\n",
    "## Need:\n",
    "- simulated zeros paths (from main_LMM_SABR) \\\n",
    "generate the amount of paths you want and load the paths here\n",
    "- berm_swaptions class (from berm_swaptions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import scipy\n",
    "from sklearn import model_selection, metrics\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import date\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "from ray.tune.suggest.skopt import SkOptSearch\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from berm_swaption_class import * #import berm_swaptions & berm_swaptions_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load rates sample dataset\n",
    "'''\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "path_rates = os.path.join(os.getcwd(), \"simulation_results\")\n",
    "path_valuation_results = os.path.join(os.getcwd(), \"valuation_results\")\n",
    "path_tune_chk = os.path.join(os.getcwd(), \"tune_checkpoint\") #main path for tune checkpoint and trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rates_1k = load_object(path_rates+\"\\\\simulated_zeros_1k.pkl\")\n",
    "sim_rates_10k = load_object(path_rates+\"\\\\simulated_zeros_10k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 1000)\n",
      "[[0.99884244 0.         0.         0.         0.        ]\n",
      " [0.9976228  0.99854474 0.         0.         0.        ]\n",
      " [0.99739711 0.99706474 0.99810346 0.         0.        ]\n",
      " [0.99730355 0.99641432 0.99606452 0.99800189 0.        ]\n",
      " [0.99679304 0.99587095 0.99580402 0.99592028 0.99851665]]\n",
      "(20, 20, 10000)\n",
      "[[0.99884244 0.         0.         0.         0.        ]\n",
      " [0.9976228  0.99877373 0.         0.         0.        ]\n",
      " [0.99739711 0.99751846 0.99862225 0.         0.        ]\n",
      " [0.99730355 0.99725279 0.99712725 0.99849212 0.        ]\n",
      " [0.99679304 0.99639462 0.99583648 0.99571872 0.99731072]]\n"
     ]
    }
   ],
   "source": [
    "#check sample rate paths\n",
    "print(sim_rates_1k.shape)\n",
    "print(sim_rates_1k[:5,:5,0])\n",
    "\n",
    "print(sim_rates_10k.shape)\n",
    "print(sim_rates_10k[:5,:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list containing all the rates\n",
    "rate_list = [sim_rates_1k, sim_rates_10k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN Model\n",
    "Changeable number of layers/neurons for Bayesian Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# define custom neural net class for bayes opt tuning\n",
    "# =============================================================================\n",
    "class net(nn.Module):\n",
    "    def __init__(self, input_dim, neurons, hidden_layers, activation_fn):\n",
    "        super(net, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.layers= nn.ModuleList()\n",
    "        #add first layer\n",
    "        self.layers.append(nn.Linear(input_dim, neurons))\n",
    "        \n",
    "        for k in range(1, hidden_layers+1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            \n",
    "        #output layer\n",
    "        self.layers.append(nn.Linear(neurons, 1))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)-1): #except last layer\n",
    "            x = self.layers[i](x)\n",
    "            x = self.activation_fn(x)\n",
    "\n",
    "        #output layer (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laguerre polynomials for the basis functions at each step\n",
    "\n",
    "def laguerre_scipy(x, k):\n",
    "    #x: paths x nvars\n",
    "    #return paths x (nvars * polynomials)\n",
    "    \n",
    "    dim = x.shape\n",
    "    L = np.zeros((dim[0], 1)) #paths x 1 #placeholder\n",
    "    \n",
    "    for i in range(dim[1]):\n",
    "        weight = np.exp(-x[:, i]/2)\n",
    "        for j in range(k):\n",
    "            L = np.concatenate([L,\n",
    "                            (weight.reshape(-1,1) * \n",
    "                            scipy.special.eval_genlaguerre(j, 0, x[:,i]).reshape(-1,1))], axis = 1)\n",
    "    \n",
    "    L = L[:,1:]\n",
    "    return(L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training NN function using tune to report validation losses for early stopping & bayes-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN(config, x, y, input_size, max_num_epochs, use_tune = True,\n",
    "            checkpoint_dir = None):\n",
    "    '''\n",
    "    config = dict of number of neurons, num hidden, activation function used, learning rate, batch size\n",
    "    '''\n",
    "    \n",
    "    model = net(input_size, config[\"num_neurons\"], config[\"num_hidden\"], config[\"activation_fn\"])\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    criterion = torch.nn.functional.mse_loss\n",
    "    optimizer=optim.Adam(model.parameters(),lr= config[\"lr\"])\n",
    "    \n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min',\n",
    "                                     patience = 10, threshold_mode = 'rel', threshold = 1e-4)\n",
    "    \n",
    "    \n",
    "    x = x.reshape(-1, input_size)\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    #split\n",
    "    x_train,x_test,y_train,y_test = model_selection.train_test_split(x,\n",
    "                                                                    y, test_size=0.1) \n",
    "    \n",
    "    x_train = torch.tensor(x_train,dtype=torch.float)\n",
    "    y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "    x_test = torch.tensor(x_test,dtype=torch.float)\n",
    "    y_test = torch.tensor(y_test,dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(x_train.view(-1, input_size), y_train.view(-1,1))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test.view(-1, input_size), y_test.view(-1,1))\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                               batch_size= 32,  #\n",
    "                                               shuffle=True)\n",
    "\n",
    "\n",
    "    #track losses\n",
    "    train_losses, test_losses = [], []\n",
    "    test_r2 = []\n",
    "    avg_train_losses, avg_test_losses  = [], []\n",
    "    avg_test_r2  = []\n",
    "    \n",
    "    #set initial losses as inf\n",
    "    avg_train_losses.append(float('inf'))\n",
    "    avg_test_losses.append(float('inf'))\n",
    "    \n",
    "    def accuracy_r2(y_true, y_pred):\n",
    "        #for tensors\n",
    "        y_true = y_true.detach().numpy().reshape(-1,1)\n",
    "        y_pred = y_pred.detach().numpy().reshape(-1,1)\n",
    "        return metrics.r2_score(y_true, y_pred)\n",
    "    \n",
    "    for epoch in range(max_num_epochs):\n",
    "        # =======================\n",
    "        # train        \n",
    "        # =======================\n",
    "        model.train() #training mode    \n",
    "        for i, (features, price) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            features, price= features.to(device), price.to(device)\n",
    "            \n",
    "            #forward\n",
    "            model_output = model(features)\n",
    "            \n",
    "            #backprop\n",
    "            loss = criterion(model_output,price)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "            \n",
    "        # ======================\n",
    "        # eval\n",
    "        # ======================\n",
    "        model.eval() # prep model for evaluation\n",
    "        for i, (features, price) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "                features = features.to(device)\n",
    "                price = price.to(device)\n",
    "                # forward pass\n",
    "                model_output = model(features)\n",
    "                \n",
    "                # calculate the loss\n",
    "                loss = criterion(model_output, price)\n",
    "                # record validation loss\n",
    "                test_losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "        train_loss, test_loss = np.mean(train_losses), np.mean(test_losses)\n",
    "        \n",
    "\n",
    "        #whole validation set\n",
    "        test_r2 = accuracy_r2(test_dataset.tensors[1], model(test_dataset.tensors[0]))\n",
    "    \n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        avg_test_r2.append(test_r2)\n",
    "        \n",
    "        #lr scheduler min mse\n",
    "        lr_scheduler.step(test_loss)\n",
    "        \n",
    "        \n",
    "        if use_tune:\n",
    "            print(\"Saving checkpoint\")\n",
    "            with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                  path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                  torch.save(model.state_dict(), path) # optimizer.state_dict()\n",
    "                  \n",
    "            tune.report(loss= test_loss, accuracy= test_r2) #report back to tune\n",
    "            \n",
    "            \n",
    "            \n",
    "        #clear list\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "\n",
    "    print(\"Finished Training\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define main NN function that uses tune with ASHAscheduler and BayesianOpt from skopt to try different nn configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# setup environament for ray tune\n",
    "# =============================================================================\n",
    "'''\n",
    "config: dictionary of search space\n",
    "num_neurons = #of neurons for each hidden layer\n",
    "num_hidden = number of hidden layers (not incl. output layer)\n",
    "activation_fn = activation function for each hidden layer\n",
    "batch size = batch size for mini batch GD\n",
    "lr = learning rate for adam\n",
    "'''\n",
    "def main_NN(data, num_samples=15, max_num_epochs=30,\n",
    "            metric = 'loss', mode = 'min',\n",
    "            checkpoint_dir = None,\n",
    "            trial_path = None,\n",
    "            experiment_name = \"experiment\"+str(date.today()),\n",
    "            trial_name = None):\n",
    "    '''\n",
    "    data = tuple of (X, y) dataset used for training and validation\n",
    "    num_samples = samples to search from search space\n",
    "    max_num_epochs = max number of epochs to train the NN\n",
    "    \n",
    "    search over NN hyperspace defined by config\n",
    "    max_num_epochs = max epochs for ASHAScheduler to terminate training\n",
    "    num_samples = num trials\n",
    "    \n",
    "    trial_name= current trial name\n",
    "    trial_dir = same as trial name\n",
    "    \n",
    "    '''\n",
    "    #if not using bayes-opt\n",
    "    \n",
    "    # config = {\n",
    "    #     \"num_neurons\": tune.choice([16, 32, 64, 128]),\n",
    "    #     \"num_hidden\": tune.choice([2,3,4]),\n",
    "    #     \"activation_fn\" : tune.choice([F.relu, F.leaky_relu]),\n",
    "    #     \"batch_size\": tune.choice([32]),\n",
    "    #     \"lr\": tune.loguniform(1e-4, 1e-1)\n",
    "    # }\n",
    "    \n",
    "    bayes_searchspace = {\n",
    "        \"num_neurons\": Categorical([int(x) for x in  2**np.arange(4,8)]),\n",
    "        \"num_hidden\": Integer(2, 4, 'uniform'),\n",
    "        \"activation_fn\" : Categorical([F.relu]),\n",
    "        # \"batch_size\": Integer(16, 32),#Categorical([int(16), int(32)]),\n",
    "        \"lr\": Real(1e-4, 1e-2, 'log-uniform')\n",
    "        }\n",
    "    \n",
    "    \n",
    "    skopt_search = SkOptSearch(space = bayes_searchspace, metric=\"accuracy\", mode=\"max\")\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    can set metric/mode in scheduler or tune.run\n",
    "    ASHA scheduler can set max_num_epochs\n",
    "    grace_period = min number of iterations before stopping\n",
    "    '''\n",
    "    scheduler = ASHAScheduler(\n",
    "                    metric= metric, #loss\n",
    "                    mode= mode, #min\n",
    "                    time_attr='training_iteration',\n",
    "                    max_t=max_num_epochs,\n",
    "                    grace_period=10, #set grace period before stopping here\n",
    "                    reduction_factor=2)\n",
    "    \n",
    "    '''\n",
    "    CLIReporter for python console\n",
    "    what to print to console\n",
    "    '''\n",
    "    reporter = CLIReporter( \n",
    "        parameter_columns=[\"lr\", \"num_neurons\", \"num_hidden\"],\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    #get dataset\n",
    "    x, y = data\n",
    "    input_size = x.shape[1]\n",
    "    \n",
    "    \n",
    "    def trial_name_string(trial):\n",
    "        return trial_name+str(trial.trial_id)\n",
    "    \n",
    "    result = tune.run(\n",
    "            partial(trainNN, x = x, y = y, input_size = input_size, max_num_epochs = max_num_epochs,\n",
    "                    checkpoint_dir = checkpoint_dir),\n",
    "            resources_per_trial={\"cpu\": 3, \"gpu\": 0},\n",
    "            # config=config,\n",
    "            search_alg = skopt_search,\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "            reuse_actors = True,\n",
    "            progress_reporter=reporter,\n",
    "            name= experiment_name,\n",
    "            local_dir = trial_path,\n",
    "            trial_name_creator = trial_name_string,\n",
    "            trial_dirname_creator = trial_name_string)\n",
    "    \n",
    "    #get best trial\n",
    "    best_trial = result.get_best_trial(metric, mode, \"last\") #\"accuracy\" , max\n",
    "    \n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "    \n",
    "    best_trained_model = net(input_size, best_trial.config[\"num_neurons\"],\n",
    "                             best_trial.config[\"num_hidden\"], best_trial.config[\"activation_fn\"])\n",
    "    \n",
    "    \n",
    "    #load best model state dict and optimizer from best checkpoint path\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    \n",
    "    model_state  = torch.load(os.path.join( #optimizer_state\n",
    "         checkpoint_dir, \"checkpoint\"))\n",
    "    \n",
    "    \n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    \n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        # if gpus_per_trial > 1:\n",
    "        #     best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "    \n",
    "    \n",
    "    return best_trained_model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with Neural Network\n",
    "Set up environment and train hyperparameters for nn using config\n",
    "Use simulated interest rate model\n",
    "loop through exercise steps to calculate continuation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wrapper function for neural network framework to calculate out-of-sample results\n",
    "'''\n",
    "def oos_nn(berm_swaptions, model, step, laguerre = False):\n",
    "    '''\n",
    "    berm_swaptions = OOS bermudan swaptions class\n",
    "    model = neural net model\n",
    "    '''\n",
    "    itm = np.where(berm_swaptions.intrinsic[:, step] > 0)[0]\n",
    "    \n",
    "    X = berm_swaptions.X(step)[0]\n",
    "    initial_dim = X.shape\n",
    "    \n",
    "    X = X[itm, :]\n",
    "    \n",
    "    if laguerre:\n",
    "        X = laguerre_scipy(X, 3)\n",
    "    \n",
    "    y = berm_swaptions.value[:, step+1]\n",
    "    y = berm_swaptions.params[\"rate_matrix\"][step, step, itm] * y[itm] #discount to now\n",
    "    \n",
    "    pred = model(torch.tensor(X, dtype = torch.float)).detach().numpy().reshape(-1,1)\n",
    "    \n",
    "    pred_all = np.zeros((initial_dim[0], 1))\n",
    "    pred_all[itm, :] = pred\n",
    "    \n",
    "    #update\n",
    "    berm_swaptions.update(step, pred_all.ravel())\n",
    "    \n",
    "    #oos r2\n",
    "    r2_score = metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1))\n",
    "    return(r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "main valuation function for neural network\n",
    "'''\n",
    "def berm_nn(berm_swaptions, sim_rates, strike, lockout, tenor, opt_type = \"rec\", max_num_epochs = 30,\n",
    "                num_samples = 10, metric = 'loss', mode = 'min',\n",
    "                test_size = 0.1, seed = 123, trial_path = None, checkpoint_dir = None):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    max_num_epochs = maximum number of epochs before stopping\n",
    "    num_samples = number of samples taken for bayesopt\n",
    "    opt_type = \"rec\"/\"pay\" for receiver or payer\n",
    "    sim_rates : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "      \n",
    "    tune_result_list = []\n",
    "    model_list = []\n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    sampling_increase = 0#np.cumsum(steps%3)\n",
    "    total_time = 0\n",
    "    ct = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # NN model\n",
    "    \n",
    "   \n",
    "        model, tune_result = main_NN((X, y),\n",
    "                                    num_samples = int(num_samples + 0),\n",
    "                                    max_num_epochs = max_num_epochs,\n",
    "                           trial_path = trial_path+\"\\\\trials\",\n",
    "                           checkpoint_dir = checkpoint_dir,\n",
    "                          experiment_name = \"nn_step_\"+str(i), trial_name = \"nn\",\n",
    "                           metric = metric, mode = mode)\n",
    "        \n",
    "        tune_result.get_best_trial('loss', 'min', 'last').config\n",
    "        \n",
    "        \n",
    "        tune_result_list.append(tune_result)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        #valuation\n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += (t1 - t0)\n",
    "        \n",
    "        pred = model(torch.tensor(X, dtype =  torch.float)).detach().numpy().reshape(-1,1)\n",
    "        pred_all = np.zeros((training_paths , 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "\n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        print(\"Step: \" + str(i))\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        \n",
    "        \n",
    "        oos_r2[\"Accuracy\"].append(oos_nn(berm_swaptions = sim_oos,\n",
    "                                           model = model,\n",
    "                                           step = i, laguerre = True)) #append r2 and update\n",
    "        ct += 1\n",
    "        \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": steps,\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, steps], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, steps], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    print(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    print(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    print(price_oos)\n",
    "    \n",
    "    return (price, price_oos, results_table, total_time, model_list, tune_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-22 00:25:21,718\tWARNING experiment.py:274 -- No name detected on trainable. Using DEFAULT.\n",
      "2020-12-22 00:25:21,718\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n",
      "2020-12-22 00:25:21,774\tWARNING trial_runner.py:580 -- Trial Runner checkpointing failed: can't pickle dict_values objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 11.8/15.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 20.000: None | Iter 10.000: None\n",
      "Resources requested: 3/12 CPUs, 0/1 GPUs, 0.0/2.93 GiB heap, 0.0/0.98 GiB objects\n",
      "Result logdir: C:\\Users\\nhian\\Dropbox\\My PC (DESKTOP-L6D69LH)\\Documents\\GitHub\\LMM_SABR\\tune_checkpoint\\trials\\nn_step_18\n",
      "Number of trials: 1/3 (1 RUNNING)\n",
      "+--------------+----------+-------+------------+---------------+--------------+\n",
      "| Trial name   | status   | loc   |         lr |   num_neurons |   num_hidden |\n",
      "|--------------+----------+-------+------------+---------------+--------------|\n",
      "| nn3b7fe61c   | RUNNING  |       | 0.00391528 |            64 |            2 |\n",
      "+--------------+----------+-------+------------+---------------+--------------+\n",
      "\n",
      "\n",
      "Result for nn3b7fe61c:\n",
      "\u001b[2m\u001b[36m(pid=44612)\u001b[0m Saving checkpoint\n",
      "\u001b[2m\u001b[36m(pid=50104)\u001b[0m Saving checkpoint\n",
      "  accuracy: -32.20822828008994\n",
      "  date: 2020-12-22_00-25-29\n",
      "  done: false\n",
      "  experiment_id: 3962d417ccdc4a58a34c74e0507b7f9e\n",
      "  experiment_tag: 1_activation_fn=<function relu at 0x000001BAC2C4A8B8>,lr=0.0039153,num_hidden=2,num_neurons=64\n",
      "  hostname: DESKTOP-L6D69LH\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.0008293602732010186\n",
      "  node_ip: 192.168.1.102\n",
      "  pid: 44612\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.18300557136535645\n",
      "  time_this_iter_s: 0.18300557136535645\n",
      "  time_total_s: 0.18300557136535645\n",
      "  timestamp: 1608625529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b7fe61c\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 12.1/15.7 GiB\n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 20.000: None | Iter 10.000: None\n",
      "Resources requested: 9/12 CPUs, 0/1 GPUs, 0.0/2.93 GiB heap, 0.0/0.98 GiB objects\n",
      "Result logdir: C:\\Users\\nhian\\Dropbox\\My PC (DESKTOP-L6D69LH)\\Documents\\GitHub\\LMM_SABR\\tune_checkpoint\\trials\\nn_step_18\n",
      "Number of trials: 3/3 (3 RUNNING)\n",
      "+--------------+----------+---------------------+-------------+---------------+--------------+------------+------------+----------------------+\n",
      "| Trial name   | status   | loc                 |          lr |   num_neurons |   num_hidden |       loss |   accuracy |   training_iteration |\n",
      "|--------------+----------+---------------------+-------------+---------------+--------------+------------+------------+----------------------|\n",
      "| nn3b7fe61c   | RUNNING  | 192.168.1.102:44612 | 0.00391528  |            64 |            2 | 0.00082936 |   -32.2082 |                    1 |\n",
      "| nn3b87393a   | RUNNING  |                     | 0.000141241 |            32 |            3 |            |            |                      |\n",
      "| nn3b8b57ee   | RUNNING  |                     | 0.00304641  |            64 |            3 |            |            |                      |\n",
      "+--------------+----------+---------------------+-------------+---------------+--------------+------------+------------+----------------------+\n",
      "\n",
      "\n",
      "Result for nn3b87393a:\n",
      "  accuracy: -220.18441528859128\n",
      "  date: 2020-12-22_00-25-29\n",
      "  done: false\n",
      "  experiment_id: 5edf6e9dd2b1420bb777fc21b90968c1\n",
      "  experiment_tag: 2_activation_fn=<function relu at 0x000001BAC2C4A8B8>,lr=0.00014124,num_hidden=3,num_neurons=32\n",
      "  hostname: DESKTOP-L6D69LH\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.002083501312881708\n",
      "  node_ip: 192.168.1.102\n",
      "  pid: 50104\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.19900059700012207\n",
      "  time_this_iter_s: 0.19900059700012207\n",
      "  time_total_s: 0.19900059700012207\n",
      "  timestamp: 1608625529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b87393a\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=37352)\u001b[0m Saving checkpoint\n",
      "\u001b[2m\u001b[36m(pid=44612)\u001b[0m Saving checkpoint\n",
      "Result for nn3b8b57ee:\n",
      "  accuracy: -66.3875949202069\n",
      "  date: 2020-12-22_00-25-29\n",
      "  done: false\n",
      "  experiment_id: 959497b943e44de0b95dcf8b71d661e2\n",
      "  experiment_tag: 3_activation_fn=<function relu at 0x000001BAC2C4A8B8>,lr=0.0030464,num_hidden=3,num_neurons=64\n",
      "  hostname: DESKTOP-L6D69LH\n",
      "  iterations_since_restore: 1\n",
      "  loss: 0.0007252037175931036\n",
      "  node_ip: 192.168.1.102\n",
      "  pid: 37352\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 0.2429943084716797\n",
      "  time_this_iter_s: 0.2429943084716797\n",
      "  time_total_s: 0.2429943084716797\n",
      "  timestamp: 1608625529\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 3b8b57ee\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=50104)\u001b[0m Saving checkpoint\n",
      "\u001b[2m\u001b[36m(pid=44612)\u001b[0m Saving checkpoint\n",
      "\u001b[2m\u001b[36m(pid=37352)\u001b[0m Saving checkpoint\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-289e6dc52652>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                \u001b[0mtenor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rec\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m123\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_tune_chk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                checkpoint_dir = path_tune_chk)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-69-83beaf7339d3>\u001b[0m in \u001b[0;36mberm_nn\u001b[1;34m(berm_swaptions, sim_rates, strike, lockout, tenor, opt_type, max_num_epochs, num_samples, metric, mode, test_size, seed, trial_path, checkpoint_dir)\u001b[0m\n\u001b[0;32m     77\u001b[0m                            \u001b[0mcheckpoint_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                           \u001b[0mexperiment_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nn_step_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nn\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                            metric = metric, mode = mode)\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mtune_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'last'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-50eddb00a8b6>\u001b[0m in \u001b[0;36mmain_NN\u001b[1;34m(data, num_samples, max_num_epochs, metric, mode, checkpoint_dir, trial_path, experiment_name, trial_name)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mlocal_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mtrial_name_creator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial_name_string\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             trial_dirname_creator = trial_name_string)\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m#get best trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\tune.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m                     trial=next_trial)\n\u001b[0;32m    569\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    709\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process_trial\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\trial_runner.py\u001b[0m in \u001b[0;36m_process_trial\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m    775\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_duplicate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m                 trial.update_last_result(\n\u001b[1;32m--> 777\u001b[1;33m                     result, terminate=(decision == TrialScheduler.STOP))\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# Checkpoints to disk. This should be checked even if\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\trial.py\u001b[0m in \u001b[0;36mupdate_last_result\u001b[1;34m(self, result, terminate)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_update_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflatten_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\logger.py\u001b[0m in \u001b[0;36mon_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_logger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m             \u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_syncer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_worker_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNODE_IP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_syncer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_down_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\ray\\tune\\logger.py\u001b[0m in \u001b[0;36mon_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[0mvalid_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfull_attr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 self._file_writer.add_scalar(\n\u001b[1;32m--> 225\u001b[1;33m                     full_attr, value, global_step=step)\n\u001b[0m\u001b[0;32m    226\u001b[0m             elif (type(value) == list\n\u001b[0;32m    227\u001b[0m                   \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py\u001b[0m in \u001b[0;36madd_scalar\u001b[1;34m(self, tag, scalar_value, global_step, walltime, display_name, summary_description)\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input value: \\\"{}\\\" is not a scalar\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         self._get_file_writer().add_summary(\n\u001b[1;32m--> 412\u001b[1;33m             scalar(tag, scalar_value, display_name, summary_description), global_step, walltime)\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_scalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_tag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py\u001b[0m in \u001b[0;36madd_summary\u001b[1;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m    134\u001b[0m         \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_profile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboardX\\writer.py\u001b[0m in \u001b[0;36madd_event\u001b[1;34m(self, event, step, walltime)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# Make sure step is converted from numpy or other formats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# since protobuf might not convert depending on version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mfield_setter\u001b[1;34m(self, new_value)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[1;31m# (0, 0.0, enum 0, and False).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m       \u001b[0mnew_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m       raise TypeError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\u001b[0m in \u001b[0;36mCheckValue\u001b[1;34m(self, proposed_value)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mCheckValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproposed_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproposed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m       message = ('%.1024r has type %s, but expected one of: %s' %\n\u001b[0;32m    170\u001b[0m                  (proposed_value, type(proposed_value), six.integer_types))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\abc.py\u001b[0m in \u001b[0;36m__subclasscheck__\u001b[1;34m(cls, subclass)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;34m\"\"\"Override for issubclass(subclass, cls).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_abc_subclasscheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_dump_registry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m F1222 00:29:55.508370 26140 39084 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 2\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCC8052B  public: void __cdecl google::LogMessage::Flush(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCC7F382  public: __cdecl google::LogMessage::~LogMessage(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCC51628  public: virtual __cdecl google::NullStreamFatal::~NullStreamFatal(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCB8E3C4  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCB8D507  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCB8CF54  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCB890BC  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCAC1D3E  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCACBC41  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCA8803A  public: class google::LogMessageVoidify & __ptr64 __cdecl google::LogMessageVoidify::operator=(class google::LogMessageVoidify const & __ptr64) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6DCF03250  bool __cdecl google::Demangle(char const * __ptr64,char * __ptr64,int)\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFF08B67C24  BaseThreadInitThunk\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFF0928D4D1  RtlUserThreadStart\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m F1222 00:29:55.771371 43392 29496 redis_client.cc:74]  Check failed: num_attempts < RayConfig::instance().redis_db_connect_retries() Expected 1 Redis shard addresses, found 2\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m *** Check failure stack trace: ***\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0CE7E9B  public: void __cdecl google::LogMessage::Flush(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0CE6CF2  public: __cdecl google::LogMessage::~LogMessage(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0CAAE18  public: virtual __cdecl google::NullStreamFatal::~NullStreamFatal(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0B83294  public: class google::LogMessage::LogStream * __ptr64 __cdecl google::LogMessage::LogStream::self(void)const __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0B823D7  public: class google::LogMessage::LogStream * __ptr64 __cdecl google::LogMessage::LogStream::self(void)const __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0B81E24  public: class google::LogMessage::LogStream * __ptr64 __cdecl google::LogMessage::LogStream::self(void)const __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0B79ABC  public: class google::LogMessage::LogStream * __ptr64 __cdecl google::LogMessage::LogStream::self(void)const __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0B2FAE7  public: class google::LogMessage::LogStream * __ptr64 __cdecl google::LogMessage::LogStream::self(void)const __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0A24AC6  public: class google::NullStream & __ptr64 __cdecl google::NullStream::stream(void) __ptr64\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FF6B0F86E90  bool __cdecl google::Demangle(char const * __ptr64,char * __ptr64,int)\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFF08B67C24  BaseThreadInitThunk\n",
      "\u001b[2m\u001b[36m(pid=None)\u001b[0m     @   00007FFF0928D4D1  RtlUserThreadStart\n"
     ]
    }
   ],
   "source": [
    "berm_nn(berm_swaptions, sim_rates_1k, strike = 0.0015, lockout = 1, #1 into 10, 0.0015 as strike\n",
    "                               tenor = 10, opt_type = \"rec\",\n",
    "                               num_samples = 3, max_num_epochs = 25, test_size = 0.1, seed = 123, trial_path = path_tune_chk,\n",
    "                               checkpoint_dir = path_tune_chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "value bermudan swaptions using NN at each step from the rate_list\n",
    "'''\n",
    "nn_results_1_10 = []\n",
    "for i in rate_list:\n",
    "    nn_results_1_10.append(berm_nn(berm_swaptions, i, strike = 0.0015, lockout = 1, #1 into 10, 0.0015 as strike\n",
    "                               tenor = 10, opt_type = \"rec\",\n",
    "                               num_samples = 3, max_num_epochs = 25, test_size = 0.1, seed = 123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with Linear Regression\n",
    "### For Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper function for calculation OOS values for XGB and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oos_pred(berm_swaptions, model, step, laguerre = False):\n",
    "    '''\n",
    "    berm_swaptions = OOS bermudan swaptions class\n",
    "    model = out model with predict method (X,y)\n",
    "    '''\n",
    "    itm = np.where(berm_swaptions.intrinsic[:, step] > 0)[0]\n",
    "    \n",
    "    X = berm_swaptions.X(step)[0]\n",
    "    initial_dim = X.shape\n",
    "    \n",
    "    X = X[itm, :]\n",
    "    \n",
    "    if laguerre:\n",
    "        X = laguerre_scipy(X, 3)\n",
    "    \n",
    "    y = berm_swaptions.value[:, step+1]\n",
    "    y = berm_swaptions.params[\"rate_matrix\"][step, step, itm] * y[itm] #discount to now\n",
    "    \n",
    "    pred = model.predict(X).reshape(-1,1)\n",
    "    \n",
    "    pred_all = np.zeros((initial_dim[0], 1))\n",
    "    pred_all[itm, :] = pred\n",
    "    \n",
    "    #update\n",
    "    berm_swaptions.update(step, pred_all.ravel())\n",
    "    \n",
    "    #oos r2\n",
    "    r2_score = metrics.r2_score(pred.reshape(-1,1), y.reshape(-1,1))\n",
    "    return(r2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berm_lr(berm_swaptions, sim_rates, strike, lockout, tenor, opt_type = \"rec\",\n",
    "                test_size = 0.1, seed = 123, verbose = False):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    sim_zeros : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout, opt_type = opt_type)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "      \n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    total_time = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # model\n",
    "        \n",
    "        out = LinearRegression().fit(X, y.reshape(-1,1))\n",
    "        pred= out.predict(X).reshape(-1,1)\n",
    "        \n",
    "        pred_all = np.zeros((training_paths , 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += (t1 - t0)\n",
    "        \n",
    "        \n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(out.predict(X).reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        verboseprint(\"Step: \" + str(i))\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        oos_r2[\"Accuracy\"].append(oos_pred(berm_swaptions = sim_oos,\n",
    "                                           model = out,\n",
    "                                           step = i, laguerre = True)) #append r2 and update\n",
    "        \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": steps,\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, steps], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, steps], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    verboseprint(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    verboseprint(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    verboseprint(price_oos)\n",
    "    \n",
    "    return (price, price_oos, results_table, total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LR simulation for all rate paths contained in rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results_1_10 = []\n",
    "\n",
    "for i in rate_list:\n",
    "    lr_results_1_10.append(berm_lr(berm_swaptions = berm_swaptions,\n",
    "                              sim_rates = i, strike = 0.0015, lockout = 1, tenor = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bermudan Swaption Valuations with XGBoost\n",
    "### Bayesian Opt using skopt\n",
    "Bayesian optimization at each step \\\n",
    "Takes considerable amount of time to run ~1-2 hours depending on number of paths and steps estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def berm_xgb(berm_swaptions, sim_rates, strike, lockout, tenor, test_size = 0.1,\n",
    "             search_iter = 15, cv = 3, search_scoring = 'neg_mean_squared_error',\n",
    "             seed = 123, verbose = False):\n",
    "    '''\n",
    "    berm_swaptions: class\n",
    "    sim_zeros : simulated zeros matrix n x n x paths\n",
    "    strike: strike for the swaption\n",
    "    lockout: in years\n",
    "    tenor: in years\n",
    "    test_size: float to split the dataset to training-testing split\n",
    "    seed: seed for splitting\n",
    "    '''\n",
    "    verboseprint = print if verbose else lambda *a, **k: None\n",
    "    \n",
    "    total_dim = sim_rates.shape\n",
    "    total_paths = total_dim[2] #no paths\n",
    "    \n",
    "    testing_paths = int(test_size * total_paths)\n",
    "    training_paths = total_paths - testing_paths\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    testing_idx= np.sort(np.random.choice(np.arange(total_paths), testing_paths ,replace = False))\n",
    "    training_idx = np.setdiff1d(np.arange(total_paths), testing_idx)\n",
    "    \n",
    "    sim = berm_swaptions(sim_rates[:, :, training_idx],\n",
    "          strike = strike, tenor = tenor, lockout = lockout)\n",
    "    \n",
    "    #out of sample\n",
    "    sim_oos = berm_swaptions(sim_rates[:, :, testing_idx],\n",
    "                             strike = strike, tenor = tenor, lockout = lockout)\n",
    "    \n",
    "    steps = sim.exercisable_steps\n",
    "    steps = np.flip(steps)[1:]\n",
    "     \n",
    "    model_list = []\n",
    "    \n",
    "    best_results_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    oos_r2 = {\"Step\":[], \"Accuracy\":[]}\n",
    "    \n",
    "    total_time = 0\n",
    "    for i in steps:\n",
    "        '''\n",
    "        #at each time step t_{M-1} compute V_{t_{M-1}} = max(h_{tm-1}, Q_{tm-1})\n",
    "        #where Q is the expected continuation value\n",
    "        #V will be used as an output for training the NN\n",
    "        \n",
    "        #first calculate Q at tm-1\n",
    "        #which is the discounted value of G at tm\n",
    "        #G is the approximated function from the neural network    \n",
    "        '''\n",
    "        itm = np.where(sim.intrinsic[:, i] > 0)[0]\n",
    "        \n",
    "        X = sim.X(i)[0]\n",
    "        X = X[itm, :]\n",
    "        \n",
    "        X = laguerre_scipy(X, 3)\n",
    "        \n",
    "        y = sim.value[:, i+1]\n",
    "        y = sim.params[\"rate_matrix\"][i, i, itm] * y[itm] #discount to now\n",
    "        \n",
    "        t0 = time.time()\n",
    "        #===================================================================================\n",
    "        # model\n",
    "        \n",
    "        model = XGBRegressor()\n",
    "        \n",
    "        param_test = {\n",
    "                'learning_rate': Real(0.01, 0.75, 'log-uniform'),\n",
    "                'min_child_weight': Integer(0, 10, 'uniform'),\n",
    "                'max_depth': Integer(3, 35, 'uniform'),\n",
    "                'max_delta_step': Integer(0, 20),\n",
    "                'subsample': Real(0.1, 1.0, 'uniform'),\n",
    "                'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "                'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n",
    "                'reg_lambda': Real(1e-9, 10, 'log-uniform'),\n",
    "                'reg_alpha': Real(1e-9, 1e-2, 'log-uniform'),\n",
    "                'gamma': Real(1e-9, 1e-3, 'log-uniform'), # minsplit loss\n",
    "                'n_estimators': Integer(125, 350)\n",
    "                }\n",
    "        \n",
    "        \n",
    "        gsearch = BayesSearchCV(estimator = model, n_iter = search_iter,\n",
    "                              search_spaces= param_test,\n",
    "                              scoring= search_scoring, cv=cv, refit = True, random_state = seed)\n",
    "        \n",
    "        \n",
    "        search_res = gsearch.fit(X, y)\n",
    "        out = gsearch.best_estimator_ #XGBRegressor(**gsearch.best_params_).fit(X,y)\n",
    "        \n",
    "        model_list.append(gsearch.best_params_)\n",
    "        \n",
    "        pred = out.predict(X).reshape(-1,1)\n",
    "        pred_all = np.zeros((training_paths, 1))\n",
    "        pred_all[itm, :] = pred\n",
    "        \n",
    "        #===================================================================================\n",
    "        t1 = time.time()\n",
    "        total_time += t1 - t0\n",
    "        \n",
    "        best_results_r2[\"Step\"].append(i)\n",
    "        best_results_r2[\"Accuracy\"].append(metrics.r2_score(out.predict(X).reshape(-1,1), y.reshape(-1,1)))\n",
    "        \n",
    "        #update\n",
    "        sim.update(i, pred_all.ravel())\n",
    "        \n",
    "        verboseprint(\"Step: \" + str(i))\n",
    "        verboseprint(best_results_r2[\"Accuracy\"][-1])\n",
    "        \n",
    "        # =============================================================================\n",
    "        # OOS Test\n",
    "        # =============================================================================\n",
    "        oos_r2[\"Step\"].append(i)\n",
    "        #update\n",
    "        oos_r2[\"Accuracy\"].append(oos_pred(berm_swaptions = sim_oos,\n",
    "                                           model = out,\n",
    "                                           step = i, laguerre = True)) #append r2 and update, use laguerre expansion\n",
    "    \n",
    "    \n",
    "    \n",
    "    results_table = pd.DataFrame({\"Step\": np.flip(steps),\n",
    "                                 \"Training R2\": best_results_r2[\"Accuracy\"],\n",
    "                                 \"OOS R2\": oos_r2[\"Accuracy\"],\n",
    "                                  \"Exc Pr\": np.sum(sim.index[:, np.flip(steps)], axis = 0)/training_paths,\n",
    "                                  \"Exc Pr OOS\": np.sum(sim_oos.index[:, np.flip(steps)], axis = 0)/testing_paths},\n",
    "                                 )\n",
    "    \n",
    "   \n",
    "    verboseprint(results_table)\n",
    "    \n",
    "    price = np.sum(sim.cmmf[:, :-1] * np.multiply(sim.index[:,1:], sim.value[:,1:]))/training_paths \n",
    "    verboseprint(price)\n",
    "    \n",
    "    price_oos = np.sum(sim_oos.cmmf[:, :-1] * np.multiply(sim_oos.index[:,1:], sim_oos.value[:,1:]))/testing_paths\n",
    "    verboseprint(price_oos)\n",
    "    \n",
    "    \n",
    "    return (price, price_oos, results_table, total_time, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18\n",
      "0.9852836064910683\n",
      "Step: 17\n",
      "0.9949699280563477\n",
      "Step: 16\n",
      "0.995714533890012\n",
      "Step: 15\n",
      "0.9961352470847176\n",
      "Step: 14\n",
      "0.9962072898793132\n",
      "Step: 13\n",
      "0.9960149059095496\n",
      "Step: 12\n",
      "0.9949672179097943\n",
      "Step: 11\n",
      "0.9942850099635865\n",
      "Step: 10\n",
      "0.9959695496760629\n",
      "Step: 9\n",
      "0.7666032549031567\n",
      "Step: 8\n",
      "0.2174570012412309\n",
      "Step: 7\n",
      "-0.028553349402233863\n",
      "Step: 6\n",
      "-0.32595587531823433\n",
      "Step: 5\n",
      "0.6584164867068752\n",
      "Step: 4\n",
      "-0.8496527159560869\n",
      "Step: 3\n",
      "-1.8287200925857743\n",
      "Step: 2\n",
      "-1.4413601802637395\n",
      "    Step  Training R2    OOS R2    Exc Pr  Exc Pr OOS\n",
      "0      2     0.985284  0.962016  0.218889        0.17\n",
      "1      3     0.994970  0.983059  0.122222        0.12\n",
      "2      4     0.995715  0.949316  0.018889        0.02\n",
      "3      5     0.996135  0.895410  0.075556        0.10\n",
      "4      6     0.996207  0.771292  0.076667        0.02\n",
      "5      7     0.996015  0.678754  0.010000        0.01\n",
      "6      8     0.994967  0.416784  0.003333        0.01\n",
      "7      9     0.994285 -0.014584  0.020000        0.00\n",
      "8     10     0.995970  0.222887  0.027778        0.05\n",
      "9     11     0.766603  0.060807  0.008889        0.00\n",
      "10    12     0.217457 -0.465758  0.017778        0.02\n",
      "11    13    -0.028553 -0.618705  0.012222        0.00\n",
      "12    14    -0.325956 -1.652677  0.011111        0.00\n",
      "13    15     0.658416 -0.440217  0.011111        0.01\n",
      "14    16    -0.849653 -1.105003  0.007778        0.02\n",
      "15    17    -1.828720 -1.573564  0.012222        0.00\n",
      "16    18    -1.441360 -2.215862  0.025556        0.02\n",
      "0.005138202794209562\n",
      "0.0028912948803467963\n",
      "Step: 18\n",
      "0.9831848006029723\n",
      "Step: 17\n",
      "0.9971103377963453\n",
      "Step: 16\n",
      "0.9961624670315802\n",
      "Step: 15\n",
      "0.9832018018334138\n",
      "Step: 14\n",
      "0.9658966354040263\n",
      "Step: 13\n",
      "0.9309041063048462\n",
      "Step: 12\n",
      "0.8798764254257994\n",
      "Step: 11\n",
      "0.8253463791823573\n",
      "Step: 10\n",
      "0.660280453694524\n",
      "Step: 9\n",
      "0.5786598900326165\n",
      "Step: 8\n",
      "0.48835484701265175\n",
      "Step: 7\n",
      "0.4228312697193153\n",
      "Step: 6\n",
      "0.25479989355666655\n",
      "Step: 5\n",
      "0.15747617956869053\n",
      "Step: 4\n",
      "-0.04830518988331112\n",
      "Step: 3\n",
      "-0.3668926520299418\n",
      "Step: 2\n",
      "-0.7404306832081762\n",
      "    Step  Training R2    OOS R2    Exc Pr  Exc Pr OOS\n",
      "0      2     0.983185  0.959955  0.268889       0.209\n",
      "1      3     0.997110  0.986734  0.174333       0.175\n",
      "2      4     0.996162  0.984414  0.074000       0.106\n",
      "3      5     0.983202  0.968400  0.015111       0.016\n",
      "4      6     0.965897  0.933858  0.049000       0.050\n",
      "5      7     0.930904  0.899513  0.012556       0.021\n",
      "6      8     0.879876  0.819671  0.020333       0.023\n",
      "7      9     0.825346  0.719726  0.006000       0.005\n",
      "8     10     0.660280  0.605128  0.012333       0.020\n",
      "9     11     0.578660  0.439547  0.005000       0.007\n",
      "10    12     0.488355  0.349486  0.016333       0.011\n",
      "11    13     0.422831  0.241395  0.004111       0.003\n",
      "12    14     0.254800  0.072496  0.014222       0.015\n",
      "13    15     0.157476 -0.193906  0.005556       0.002\n",
      "14    16    -0.048305 -0.222335  0.008556       0.009\n",
      "15    17    -0.366893 -0.417866  0.005222       0.009\n",
      "16    18    -0.740431 -0.760337  0.026667       0.013\n",
      "0.004816259307637045\n",
      "0.0041849371230658974\n"
     ]
    }
   ],
   "source": [
    "xgb_results_1_10 = []\n",
    "\n",
    "for i in rate_list:\n",
    "    xgb_results_1_10.append(berm_xgb(berm_swaptions = berm_swaptions,\n",
    "                              sim_rates = i, strike = 0.0015, lockout = 1, tenor = 10, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save run results\n",
    "save_object(lr_results_1_10, path_valuation_results+\"\\\\lr_results_1_10.pkl\")\n",
    "\n",
    "save_object(nn_results_1_10, path_valuation_results+\"\\\\nn_results_1_10.pkl\")\n",
    "\n",
    "save_object(xgb_results_1_10, path_valuation_results+\"\\\\xgb_results_1_10.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
